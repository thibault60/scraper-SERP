import streamlit as st
import pandas as pd

# Import SerpAPI avec fallback legacy
try:
    from serpapi import GoogleSearch
except ImportError:
    from serpapi.google_search import GoogleSearch

@st.cache_data(show_spinner=False)
def fetch_urls(query: str, api_key: str, location: str="France", num: int=50) -> list[str]:
    params = {
        "engine": "google",
        "q": query,
        "location": location,
        "google_domain": "google.fr",
        "gl": "fr",
        "hl": "fr",
        "num": str(num),
        "api_key": api_key
    }
    search = GoogleSearch(params)
    data = search.get_dict()
    return [item["link"] for item in data.get("organic_results", []) if item.get("link")]

def main():
    st.set_page_config(page_title="Scraper SEO SerpAPI", layout="wide")
    st.title("ğŸ•·ï¸ Scraper d'URLs SEO avec SerpAPI")

    api_key = st.secrets.get("serpapi_key")
    if not api_key:
        st.error(
            "âŒ ClÃ© SerpAPI introuvable. "
            "Ajoutez `serpapi_key` dans `.streamlit/secrets.toml` "
            "ou via lâ€™UI de dÃ©ploiement."
        )
        st.stop()

    query = st.text_input("RequÃªte Google", value="site:exemple.fr")
    num   = st.slider("Nombre de rÃ©sultats", 10, 100, 50, 10)

    if st.button("ğŸš€ Lancer le scraping"):
        with st.spinner("Recherche en coursâ€¦"):
            urls = fetch_urls(query, api_key, num=num)
        if urls:
            st.success(f"{len(urls)} URLs rÃ©cupÃ©rÃ©es.")
            df = pd.DataFrame({"URL": urls})
            st.dataframe(df, use_container_width=True)
            csv = df.to_csv(index=False).encode("utf-8")
            st.download_button("â¬‡ï¸ TÃ©lÃ©charger CSV", csv, "urls.csv", "text/csv")
        else:
            st.warning("Aucun rÃ©sultat trouvÃ©.")

if __name__ == "__main__":
    main()
