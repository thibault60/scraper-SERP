# streamlit_app.py

import streamlit as st
import pandas as pd
from github import Github  # veillez Ã  avoir "PyGithub" dans requirements.txt
from serpapi import GoogleSearch
import requests

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Gestion des secrets (.streamlit/secrets.toml)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CrÃ©ez un fichier .streamlit/secrets.toml contenant :
# serpapi_key = "VOTRE_CLE_SERPAPI"
SERPAPI_KEY = st.secrets["serpapi_key"]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Lecture de queries.txt depuis GitHub (public)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
github = Github()  # accÃ¨s en lecture seule aux dÃ©pÃ´ts publics
repo = github.get_repo("thibault60/scraper-SERP")
contents = repo.get_contents("queries.txt")
raw_txt = requests.get(contents.download_url).text

# Chaque ligne de queries.txt est considÃ©rÃ©e comme une requÃªte
queries = [line.strip() for line in raw_txt.splitlines() if line.strip()]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Fonction dâ€™extraction de la position zÃ©ro
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_featured_snippet(query: str) -> dict:
    params = {
        "q": query,
        "api_key": SERPAPI_KEY,
        "hl": "fr",
        "gl": "fr",
        "num": 10,
    }
    search = GoogleSearch(params)
    results = search.get_dict()
    # SerpApi renvoie le featured snippet sous "answer_box" ou "featured_snippet"
    return results.get("answer_box") or results.get("featured_snippet") or {}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Mise en cache pour limiter les appels
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data
def fetch_snippets(queries_list: list[str]) -> pd.DataFrame:
    records = []
    for q in queries_list:
        fs = get_featured_snippet(q)
        texte = fs.get("snippet") or fs.get("answer") or "â€”"
        records.append({"RequÃªte": q, "Position zÃ©ro": texte})
    return pd.DataFrame(records)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Interface Streamlit
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Positions ZÃ©ro SERP", layout="wide")
st.title("ğŸ“Š Extraction des Positions ZÃ©ro â€“ scraper-SERP")
st.markdown(
    "Cette app lit queries.txt depuis GitHub, interroge SerpApi et affiche "
    "les featured snippets (positions zÃ©ro)."
)

if st.button("ğŸ•¹ï¸ Extraire les featured snippets"):
    df_snippets = fetch_snippets(queries)
    st.dataframe(df_snippets, use_container_width=True)

    for _, row in df_snippets.iterrows():
        st.markdown("---")
        st.subheader(f"ğŸ” {row['RequÃªte']}")
        if row["Position zÃ©ro"] != "â€”":
            st.markdown(row["Position zÃ©ro"])
        else:
            st.info("Pas de featured snippet trouvÃ© pour cette requÃªte.")
