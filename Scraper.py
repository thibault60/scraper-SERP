# Scraper.py

import streamlit as st
import pandas as pd

# Import SerpAPI (legacy fallback)
try:
    from serpapi import GoogleSearch
except ImportError:
    from serpapi.google_search import GoogleSearch

@st.cache_data(show_spinner=False)
def fetch_urls(query: str, api_key: str, location: str = "France", num: int = 50) -> list[str]:
    params = {
        "engine": "google",
        "q": query,
        "location": location,
        "google_domain": "google.fr",
        "gl": "fr",
        "hl": "fr",
        "num": str(num),
        "api_key": api_key
    }
    search = GoogleSearch(params)
    data = search.get_dict()
    return [item["link"] for item in data.get("organic_results", []) if item.get("link")]

def main():
    st.set_page_config(page_title="Scraper SEO SerpAPI", layout="wide")
    st.title("üï∑Ô∏è Scraper d'URLs SEO avec SerpAPI")

    # R√©cup√©ration s√©curis√©e de la cl√©
    api_key = st.secrets.get("serpapi_key")  # retourne None si la cl√© n'existe pas

    if not api_key:
        st.error("‚ùå Cl√© SerpAPI introuvable. "
                 "D√©clarez `serpapi_key` dans `.streamlit/secrets.toml` "
                 "ou via l‚Äôinterface de d√©ploiement.")
        st.stop()

    # Interface
    query = st.text_input("Requ√™te Google", value="site:exemple.fr")
    num   = st.slider("Nombre de r√©sultats", 10, 100, 50, 10)

    if st.button("üöÄ Lancer le scraping"):
        with st.spinner("Recherche en cours‚Ä¶"):
            urls = fetch_urls(query, api_key, num=num)
        if urls:
            st.success(f"{len(urls)} URLs r√©cup√©r√©es.")
            df = pd.DataFrame({"URL": urls})
            st.dataframe(df, use_container_width=True)
            csv = df.to_csv(index=False).encode("utf-8")
            st.download_button("‚¨áÔ∏è T√©l√©charger CSV", csv, "urls.csv", "text/csv")
        else:
            st.warning("Aucun r√©sultat organique trouv√©.")

if __name__ == "__main__":
    main()
